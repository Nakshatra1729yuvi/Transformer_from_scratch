{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "fTjCiG_eBV1D"
      },
      "outputs": [],
      "source": [
        "# !pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "import re\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "import pickle\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "import tiktoken\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "836SVI53Dzd6"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize GPT-2 tokenizer\n",
        "encoding = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "# Define the end-of-text token\n",
        "eot_token = encoding.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"})[0]\n",
        "\n",
        "text_data = [\n",
        "    \"Hello world this is GPT demo\",\n",
        "    \"I am learning transformers\",\n",
        "    \"PyTorch makes it easy\",\n",
        "    \"Causal masking is important\",\n",
        "    \"Self attention is powerful\",\n",
        "    \"Feed forward layers help\",\n",
        "    \"Normalization stabilizes training\",\n",
        "    \"Dropout prevents overfitting\",\n",
        "    \"Token embeddings are essential\",\n",
        "    \"Position embeddings add order\",\n",
        "    \"Mini GPT can learn patterns\",\n",
        "    \"Sequence modeling is fun\",\n",
        "    \"We generate text autoregressively\",\n",
        "    \"Training requires lots of data\",\n",
        "    \"Learning rate matters\",\n",
        "    \"Optimization is key\",\n",
        "    \"Batches speed up training\",\n",
        "    \"Masking future tokens is critical\",\n",
        "    \"Logits predict the next token\",\n",
        "    \"Generation loops one token at a time\",\n",
        "    \"Deep learning is fascinating\",\n",
        "    \"Neural networks are universal approximators\",\n",
        "    \"Backpropagation adjusts weights\",\n",
        "    \"Gradient descent minimizes loss\",\n",
        "    \"Overfitting occurs with small datasets\",\n",
        "    \"Validation helps detect overfitting\",\n",
        "    \"Regularization improves generalization\",\n",
        "    \"Convolutional layers process images\",\n",
        "    \"Recurrent layers process sequences\",\n",
        "    \"Transformers excel at NLP\",\n",
        "    \"Attention allows context awareness\",\n",
        "    \"GPT models are decoder-only\",\n",
        "    \"BERT models are encoder-only\",\n",
        "    \"Seq2Seq models translate languages\",\n",
        "    \"Tokenization splits text into tokens\",\n",
        "    \"Embedding layers map tokens to vectors\",\n",
        "    \"Activation functions introduce nonlinearity\",\n",
        "    \"ReLU is widely used\",\n",
        "    \"Softmax converts logits to probabilities\",\n",
        "    \"Cross entropy loss is standard for classification\",\n",
        "    \"Adam optimizer adapts learning rates\",\n",
        "    \"Learning rate schedulers help convergence\",\n",
        "    \"Gradient clipping prevents exploding gradients\",\n",
        "    \"Layer normalization stabilizes training\",\n",
        "    \"Dropout randomly disables neurons\",\n",
        "    \"Residual connections improve gradient flow\",\n",
        "    \"Positional encoding adds order information\",\n",
        "    \"Causal masking prevents cheating\",\n",
        "    \"Autoregressive models predict next token\",\n",
        "    \"Top-k sampling makes generation diverse\",\n",
        "    \"Temperature controls randomness\",\n",
        "    \"Beam search improves generation quality\",\n",
        "    \"MiniGPT is a small transformer model\",\n",
        "    \"Training takes GPU acceleration\",\n",
        "    \"Data preprocessing cleans the text\",\n",
        "    \"Padding aligns sequences\",\n",
        "    \"Batching increases efficiency\",\n",
        "    \"Evaluation measures accuracy\",\n",
        "    \"Perplexity measures language model performance\",\n",
        "    \"Text generation is fun\",\n",
        "    \"Code generation is possible\",\n",
        "    \"Mathematical reasoning can be learned\",\n",
        "    \"Logic puzzles can be solved\",\n",
        "    \"Chess and Go can be modeled\",\n",
        "    \"Reinforcement learning trains agents\",\n",
        "    \"Q-learning is a basic RL algorithm\",\n",
        "    \"Policy gradient optimizes expected reward\",\n",
        "    \"Value functions estimate future returns\",\n",
        "    \"Exploration vs exploitation is key\",\n",
        "    \"Simulation helps RL training\",\n",
        "    \"Environment defines agent interactions\",\n",
        "    \"Observations are agent inputs\",\n",
        "    \"Actions change the state\",\n",
        "    \"Rewards guide learning\",\n",
        "    \"Discount factor values future rewards\",\n",
        "    \"Experience replay stabilizes training\",\n",
        "    \"Target networks improve convergence\",\n",
        "    \"Actor-critic combines policy and value\",\n",
        "    \"Deep Q-Networks use neural networks\",\n",
        "    \"Tensor operations are efficient\",\n",
        "    \"Broadcasting simplifies arithmetic\",\n",
        "    \"Autograd computes gradients automatically\",\n",
        "    \"Checkpointing saves model states\",\n",
        "    \"Early stopping prevents overfitting\",\n",
        "    \"Hyperparameter tuning is important\",\n",
        "    \"Random seeds ensure reproducibility\",\n",
        "    \"Data augmentation expands datasets\",\n",
        "    \"Transfer learning leverages pre-trained models\",\n",
        "    \"Fine-tuning adapts models to new tasks\",\n",
        "    \"Language modeling predicts next word\",\n",
        "    \"Masked language modeling predicts missing tokens\",\n",
        "    \"Sequence classification assigns labels\",\n",
        "    \"Text summarization shortens content\",\n",
        "    \"Question answering extracts answers\",\n",
        "    \"Named entity recognition identifies entities\",\n",
        "    \"Part-of-speech tagging labels words\",\n",
        "    \"Machine translation converts languages\",\n",
        "    \"Sentiment analysis detects emotions\",\n",
        "    \"Topic modeling clusters documents\",\n",
        "    \"Clustering groups similar items\",\n",
        "    \"Dimensionality reduction simplifies data\",\n",
        "    \"Principal Component Analysis reduces dimensions\",\n",
        "    \"t-SNE visualizes high-dimensional data\",\n",
        "    \"UMAP preserves global structure\",\n",
        "    \"Cosine similarity measures similarity\",\n",
        "    \"Euclidean distance measures distance\",\n",
        "    \"KNN classifies based on neighbors\",\n",
        "    \"SVM separates classes with hyperplanes\",\n",
        "    \"Random forests ensemble decision trees\",\n",
        "    \"Gradient boosting improves weak learners\",\n",
        "    \"XGBoost is a popular boosting algorithm\",\n",
        "    \"LightGBM is optimized for speed\",\n",
        "    \"CatBoost handles categorical features\",\n",
        "    \"Neural networks approximate functions\",\n",
        "    \"Activation functions include ReLU, Tanh, Sigmoid\",\n",
        "    \"Optimization minimizes the loss function\",\n",
        "    \"Batch normalization stabilizes training\",\n",
        "    \"Residual networks improve deep training\",\n",
        "    \"Attention mechanisms focus on important features\",\n",
        "    \"Transformers replaced RNNs in NLP\",\n",
        "    \"Pre-training and fine-tuning are common\",\n",
        "    \"Self-supervised learning reduces labeled data needs\"\n",
        "]\n",
        "\n",
        "\n",
        "class GPTDataset(Dataset):\n",
        "    def __init__(self, data, block_size=32):\n",
        "        self.block_size = block_size\n",
        "        self.tokens = []\n",
        "        for line in data:\n",
        "            # Encode line and append end-of-text token\n",
        "            self.tokens.extend(encoding.encode(line) + [eot_token])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokens) - self.block_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.tensor(self.tokens[idx:idx+self.block_size], dtype=torch.long)\n",
        "        y = torch.tensor(self.tokens[idx+1:idx+self.block_size+1], dtype=torch.long)\n",
        "        return x, y\n",
        "\n",
        "\n",
        "dataset = GPTDataset(text_data, block_size=32)\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "# Example: Print the shape of input and target tensors\n",
        "for xb, yb in dataloader:\n",
        "    print(\"Input batch shape:\", xb.shape)\n",
        "    print(\"Target batch shape:\", yb.shape)\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sng1PpfbEJlI",
        "outputId": "7d395429-db45-4496-b675-e73354db32de"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input batch shape: torch.Size([2, 32])\n",
            "Target batch shape: torch.Size([2, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTConfig:\n",
        "  def __init__(self,vocab_size,block_size,n_layer=4,n_head=4,n_embd=128,dropout=0.1):\n",
        "      self.vocab_size=vocab_size\n",
        "      self.block_size=block_size\n",
        "      self.n_layer=n_layer\n",
        "      self.n_head=n_head\n",
        "      self.n_embd=n_embd\n",
        "      self.dropout=dropout"
      ],
      "metadata": {
        "id": "1_USEHSzK9_f"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CasualSelfAttention(nn.Module):\n",
        "  def __init__(self,config:GPTConfig):\n",
        "      super().__init__()\n",
        "      assert config.n_embd%config.n_head==0\n",
        "      self.n_head = config.n_head\n",
        "      self.n_embd=config.n_embd\n",
        "      self.head_dim=config.n_embd//config.n_head\n",
        "      self.c_attn=nn.Linear(config.n_embd,3*config.n_embd)\n",
        "      self.c_proj=nn.Linear(config.n_embd, config.n_embd)\n",
        "      self.attn_dropout=nn.Dropout(config.dropout)\n",
        "      self.resid_dropout=nn.Dropout(config.dropout)\n",
        "\n",
        "      self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "  def forward(self,x):\n",
        "        B,T,C=x.size()\n",
        "        q,k,v=self.c_attn(x).split(self.n_embd,dim=2)\n",
        "        q=q.view(B,T,self.n_head,self.head_dim).transpose(1,2)\n",
        "        k=k.view(B,T,self.n_head,self.head_dim).transpose(1,2)\n",
        "        v=v.view(B,T,self.n_head,self.head_dim).transpose(1,2)\n",
        "\n",
        "        att=(q@k.transpose(-2,-1))*(1.0/math.sqrt(self.head_dim))\n",
        "        att=att.masked_fill(self.bias[:,:,:T,:T]==0,float('-inf'))\n",
        "        att=F.softmax(att,dim=-1)\n",
        "        att=self.attn_dropout(att)\n",
        "\n",
        "        y=att@v\n",
        "        y=y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y=self.resid_dropout(self.c_proj(y))\n",
        "        return y\n"
      ],
      "metadata": {
        "id": "iZbfDQS8PWDQ"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, config: GPTConfig):\n",
        "        super().__init__()\n",
        "        self.c_fc=nn.Linear(config.n_embd,4*config.n_embd)\n",
        "        self.c_proj=nn.Linear(4*config.n_embd,config.n_embd)\n",
        "        self.dropout=nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.dropout(self.c_proj(F.gelu(self.c_fc(x))))\n"
      ],
      "metadata": {
        "id": "WoZSq1uWUPMg"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "  def __init__(self,config:GPTConfig):\n",
        "      super().__init__()\n",
        "      self.ln1=nn.LayerNorm(config.n_embd)\n",
        "      self.attn=CasualSelfAttention(config)\n",
        "      self.ln2=nn.LayerNorm(config.n_embd)\n",
        "      self.ff=MLP(config)\n",
        "  def forward(self,x):\n",
        "      x=x+self.attn(self.ln1(x))\n",
        "      x=x+self.ff(self.ln2(x))\n",
        "      return x"
      ],
      "metadata": {
        "id": "JCOYSzrOWEX2"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nn.Module):\n",
        "    def __init__(self, config: GPTConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        # Token + positional embeddings\n",
        "        self.tok_emb = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "        self.pos_emb = nn.Embedding(config.block_size, config.n_embd)\n",
        "        self.drop = nn.Dropout(config.dropout)\n",
        "\n",
        "        # Transformer blocks\n",
        "        self.blocks = nn.ModuleList([Block(config) for _ in range(config.n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
        "\n",
        "        # Final linear layer for logits\n",
        "        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.size()\n",
        "        assert T <= self.config.block_size, \"Sequence too long!\"\n",
        "\n",
        "        # Embeddings\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device).unsqueeze(0)\n",
        "        x = self.tok_emb(idx) + self.pos_emb(pos)\n",
        "        x = self.drop(x)\n",
        "\n",
        "        # Pass through transformer blocks\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        x = self.ln_f(x)\n",
        "\n",
        "        logits = self.head(x)\n",
        "\n",
        "        # If targets given, compute loss\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens=50, temperature=1.0, top_k=None):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -self.config.block_size:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, top_k)\n",
        "                logits[logits < v[:, [-1]]] = -float(\"Inf\")\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, next_token), dim=1)\n",
        "            if next_token.item() == eot_token:\n",
        "              break\n",
        "        return idx\n"
      ],
      "metadata": {
        "id": "L4sXs2ReZC6E"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build model\n",
        "vocab_size = encoding.n_vocab\n",
        "config = GPTConfig(vocab_size=vocab_size, block_size=32)\n",
        "model = GPT(config)\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=3e-4)\n",
        "\n",
        "# Training loop\n",
        "epochs = 3\n",
        "for epoch in range(epochs):\n",
        "    for xb, yb in tqdm(dataloader):\n",
        "        logits, loss = model(xb, yb)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63M2c6EEct3-",
        "outputId": "27183fa5-0a92-4aef-d75b-5c774d318b7a"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 422/422 [01:21<00:00,  5.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 4.1117\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 422/422 [01:20<00:00,  5.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2, Loss: 1.3834\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 422/422 [01:20<00:00,  5.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3, Loss: 0.5132\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "epochs = 3\n",
        "for epoch in range(epochs):\n",
        "    for xb, yb in tqdm(dataloader):\n",
        "        logits, loss = model(xb, yb)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNHgEpi9jbsd",
        "outputId": "93161b9a-5c1d-4b4d-cedf-170087bfc147"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 422/422 [01:20<00:00,  5.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.2207\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 422/422 [01:20<00:00,  5.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2, Loss: 0.1295\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 422/422 [01:20<00:00,  5.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3, Loss: 0.2580\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Seed prompt: \"Hello world\"\n",
        "prompt = \"Deep Neural\"\n",
        "encoded = torch.tensor([encoding.encode(prompt)], dtype=torch.long)\n",
        "\n",
        "generated = model.generate(encoded, max_new_tokens=30, temperature=0.8, top_k=50)\n",
        "decoded = encoding.decode(generated[0].tolist())\n",
        "print(\"\\nGenerated text:\\n\", decoded)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTkfMOkLcwjq",
        "outputId": "4eac7828-ea6a-4ff1-f810-d80ae6ed2b95"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated text:\n",
            " Deep Neural dimensions<|endoftext|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(\"Total parameters:\", total_params)\n"
      ],
      "metadata": {
        "id": "8hXjMD_Plztj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
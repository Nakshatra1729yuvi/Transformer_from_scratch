{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Getting Data"
      ],
      "metadata": {
        "id": "VHP_Yq3NdZ1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install tiktoken"
      ],
      "metadata": {
        "id": "oaisN6ICvGml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvtJpTNA4UMD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "import re\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "import pickle\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "import tiktoken\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize GPT-2 tokenizer\n",
        "encoding = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "# Define the end-of-text token\n",
        "eot_token = encoding.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"})[0]\n",
        "\n",
        "text_data = [\n",
        "    \"Hello world this is GPT demo\",\n",
        "    \"I am learning transformers\",\n",
        "    \"PyTorch makes it easy\",\n",
        "    \"Causal masking is important\",\n",
        "    \"Self attention is powerful\",\n",
        "    \"Feed forward layers help\",\n",
        "    \"Normalization stabilizes training\",\n",
        "    \"Dropout prevents overfitting\",\n",
        "    \"Token embeddings are essential\",\n",
        "    \"Position embeddings add order\",\n",
        "    \"Mini GPT can learn patterns\",\n",
        "    \"Sequence modeling is fun\",\n",
        "    \"We generate text autoregressively\",\n",
        "    \"Training requires lots of data\",\n",
        "    \"Learning rate matters\",\n",
        "    \"Optimization is key\",\n",
        "    \"Batches speed up training\",\n",
        "    \"Masking future tokens is critical\",\n",
        "    \"Logits predict the next token\",\n",
        "    \"Generation loops one token at a time\",\n",
        "    \"Deep learning is fascinating\",\n",
        "    \"Neural networks are universal approximators\",\n",
        "    \"Backpropagation adjusts weights\",\n",
        "    \"Gradient descent minimizes loss\",\n",
        "    \"Overfitting occurs with small datasets\",\n",
        "    \"Validation helps detect overfitting\",\n",
        "    \"Regularization improves generalization\",\n",
        "    \"Convolutional layers process images\",\n",
        "    \"Recurrent layers process sequences\",\n",
        "    \"Transformers excel at NLP\",\n",
        "    \"Attention allows context awareness\",\n",
        "    \"GPT models are decoder-only\",\n",
        "    \"BERT models are encoder-only\",\n",
        "    \"Seq2Seq models translate languages\",\n",
        "    \"Tokenization splits text into tokens\",\n",
        "    \"Embedding layers map tokens to vectors\",\n",
        "    \"Activation functions introduce nonlinearity\",\n",
        "    \"ReLU is widely used\",\n",
        "    \"Softmax converts logits to probabilities\",\n",
        "    \"Cross entropy loss is standard for classification\",\n",
        "    \"Adam optimizer adapts learning rates\",\n",
        "    \"Learning rate schedulers help convergence\",\n",
        "    \"Gradient clipping prevents exploding gradients\",\n",
        "    \"Layer normalization stabilizes training\",\n",
        "    \"Dropout randomly disables neurons\",\n",
        "    \"Residual connections improve gradient flow\",\n",
        "    \"Positional encoding adds order information\",\n",
        "    \"Causal masking prevents cheating\",\n",
        "    \"Autoregressive models predict next token\",\n",
        "    \"Top-k sampling makes generation diverse\",\n",
        "    \"Temperature controls randomness\",\n",
        "    \"Beam search improves generation quality\",\n",
        "    \"MiniGPT is a small transformer model\",\n",
        "    \"Training takes GPU acceleration\",\n",
        "    \"Data preprocessing cleans the text\",\n",
        "    \"Padding aligns sequences\",\n",
        "    \"Batching increases efficiency\",\n",
        "    \"Evaluation measures accuracy\",\n",
        "    \"Perplexity measures language model performance\",\n",
        "    \"Text generation is fun\",\n",
        "    \"Code generation is possible\",\n",
        "    \"Mathematical reasoning can be learned\",\n",
        "    \"Logic puzzles can be solved\",\n",
        "    \"Chess and Go can be modeled\",\n",
        "    \"Reinforcement learning trains agents\",\n",
        "    \"Q-learning is a basic RL algorithm\",\n",
        "    \"Policy gradient optimizes expected reward\",\n",
        "    \"Value functions estimate future returns\",\n",
        "    \"Exploration vs exploitation is key\",\n",
        "    \"Simulation helps RL training\",\n",
        "    \"Environment defines agent interactions\",\n",
        "    \"Observations are agent inputs\",\n",
        "    \"Actions change the state\",\n",
        "    \"Rewards guide learning\",\n",
        "    \"Discount factor values future rewards\",\n",
        "    \"Experience replay stabilizes training\",\n",
        "    \"Target networks improve convergence\",\n",
        "    \"Actor-critic combines policy and value\",\n",
        "    \"Deep Q-Networks use neural networks\",\n",
        "    \"Tensor operations are efficient\",\n",
        "    \"Broadcasting simplifies arithmetic\",\n",
        "    \"Autograd computes gradients automatically\",\n",
        "    \"Checkpointing saves model states\",\n",
        "    \"Early stopping prevents overfitting\",\n",
        "    \"Hyperparameter tuning is important\",\n",
        "    \"Random seeds ensure reproducibility\",\n",
        "    \"Data augmentation expands datasets\",\n",
        "    \"Transfer learning leverages pre-trained models\",\n",
        "    \"Fine-tuning adapts models to new tasks\",\n",
        "    \"Language modeling predicts next word\",\n",
        "    \"Masked language modeling predicts missing tokens\",\n",
        "    \"Sequence classification assigns labels\",\n",
        "    \"Text summarization shortens content\",\n",
        "    \"Question answering extracts answers\",\n",
        "    \"Named entity recognition identifies entities\",\n",
        "    \"Part-of-speech tagging labels words\",\n",
        "    \"Machine translation converts languages\",\n",
        "    \"Sentiment analysis detects emotions\",\n",
        "    \"Topic modeling clusters documents\",\n",
        "    \"Clustering groups similar items\",\n",
        "    \"Dimensionality reduction simplifies data\",\n",
        "    \"Principal Component Analysis reduces dimensions\",\n",
        "    \"t-SNE visualizes high-dimensional data\",\n",
        "    \"UMAP preserves global structure\",\n",
        "    \"Cosine similarity measures similarity\",\n",
        "    \"Euclidean distance measures distance\",\n",
        "    \"KNN classifies based on neighbors\",\n",
        "    \"SVM separates classes with hyperplanes\",\n",
        "    \"Random forests ensemble decision trees\",\n",
        "    \"Gradient boosting improves weak learners\",\n",
        "    \"XGBoost is a popular boosting algorithm\",\n",
        "    \"LightGBM is optimized for speed\",\n",
        "    \"CatBoost handles categorical features\",\n",
        "    \"Neural networks approximate functions\",\n",
        "    \"Activation functions include ReLU, Tanh, Sigmoid\",\n",
        "    \"Optimization minimizes the loss function\",\n",
        "    \"Batch normalization stabilizes training\",\n",
        "    \"Residual networks improve deep training\",\n",
        "    \"Attention mechanisms focus on important features\",\n",
        "    \"Transformers replaced RNNs in NLP\",\n",
        "    \"Pre-training and fine-tuning are common\",\n",
        "    \"Self-supervised learning reduces labeled data needs\"\n",
        "]\n",
        "\n",
        "\n",
        "class GPTDataset(Dataset):\n",
        "    def __init__(self, data, block_size=32):\n",
        "        self.block_size = block_size\n",
        "        self.tokens = []\n",
        "        for line in data:\n",
        "            # Encode line and append end-of-text token\n",
        "            self.tokens.extend(encoding.encode(line) + [eot_token])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokens) - self.block_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.tensor(self.tokens[idx:idx+self.block_size], dtype=torch.long)\n",
        "        y = torch.tensor(self.tokens[idx+1:idx+self.block_size+1], dtype=torch.long)\n",
        "        return x, y\n",
        "\n",
        "\n",
        "dataset = GPTDataset(text_data, block_size=32)\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "# Example: Print the shape of input and target tensors\n",
        "for xb, yb in dataloader:\n",
        "    print(\"Input batch shape:\", xb.shape)\n",
        "    print(\"Target batch shape:\", yb.shape)\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vccMCo_9b5-1",
        "outputId": "7ffdc4e3-fcbf-4f18-cf5b-76ec16fb431c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input batch shape: torch.Size([2, 32])\n",
            "Target batch shape: torch.Size([2, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "5gHVPVhRa8id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer"
      ],
      "metadata": {
        "id": "39eoXxEMdovn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attn(Q,K,V,mask=None):\n",
        "  #Q,K,V ---> (batch_size,num_head,seq,d_k)\n",
        "    d_k=Q.size(-1)\n",
        "    scores=torch.matmul(Q,K.transpose(-2,-1))/math.sqrt(d_k)\n",
        "\n",
        "    if mask is not None:\n",
        "      scores=scores.masked_fill(mask==0,-1e9)\n",
        "    attn=F.softmax(scores,dim=-1)\n",
        "    out=torch.matmul(attn,V)\n",
        "    return out,attn"
      ],
      "metadata": {
        "id": "f568zwA1doUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self,embed_size,num_head):\n",
        "    super().__init__()\n",
        "    assert embed_size%num_head==0\n",
        "    self.num_head=num_head\n",
        "    self.d_k=embed_size//num_head\n",
        "\n",
        "    self.W_Q=nn.Linear(embed_size,embed_size)\n",
        "    self.W_K=nn.Linear(embed_size,embed_size)\n",
        "    self.W_V=nn.Linear(embed_size,embed_size)\n",
        "    self.W_O=nn.Linear(embed_size,embed_size)\n",
        "  def forward(self,x,mask=None):\n",
        "    batch_size,seq_length,embed_dim=x.size()\n",
        "    Q=self.W_Q(x).view(batch_size,seq_length,self.num_head,self.d_k).transpose(1,2)\n",
        "    K=self.W_K(x).view(batch_size,seq_length,self.num_head,self.d_k).transpose(1,2)\n",
        "    V=self.W_V(x).view(batch_size,seq_length,self.num_head,self.d_k).transpose(1,2)\n",
        "\n",
        "    out,_=scaled_dot_product_attn(Q,K,V,mask)\n",
        "    out=out.transpose(1,2).reshape(batch_size,seq_length,embed_dim)\n",
        "    return self.W_O(out)"
      ],
      "metadata": {
        "id": "HK7TgJNNp1mv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self,embed_size,hidden_size):\n",
        "    super().__init__()\n",
        "    self.fc1=nn.Linear(embed_size,hidden_size)\n",
        "    self.fc2=nn.Linear(hidden_size,embed_size)\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.fc2(F.relu(self.fc1(x)))"
      ],
      "metadata": {
        "id": "JE_g9iRyx3vP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "  def __init__(self,embed_size,num_head,hidden_size_ff,dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.mha=MultiHeadAttention(embed_size,num_head)\n",
        "    self.ff=FeedForward(embed_size,hidden_size_ff)\n",
        "    self.norm1=nn.LayerNorm(embed_size)\n",
        "    self.norm2=nn.LayerNorm(embed_size)\n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "  def forward(self,x,mask=None):\n",
        "    x2=self.mha(x,mask)\n",
        "    x=self.norm1(x+self.dropout(x2))\n",
        "    x2=self.ff(x)\n",
        "    x=self.norm2(x+self.dropout(x2))\n",
        "    return x"
      ],
      "metadata": {
        "id": "kceKFdXJzURY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self,vocab_size,embed_size,max_len,num_head,hidden_size_ff,num_decoder_layer,dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.embeddings=nn.Embedding(vocab_size,embed_size)\n",
        "    self.pos_embeddings=nn.Embedding(max_len,embed_size)\n",
        "    self.layers=nn.ModuleList([DecoderLayer(embed_size,num_head,hidden_size_ff,dropout)for i in range(num_decoder_layer)])\n",
        "    self.fc_out=nn.Linear(embed_size,vocab_size)\n",
        "  def forward(self,x):\n",
        "    batch_size,seq_length=x.size()\n",
        "    positions=torch.arange(0,seq_length,device=x.device).unsqueeze(0).expand(batch_size,seq_length)\n",
        "\n",
        "    x=self.embeddings(x)+self.pos_embeddings(positions)\n",
        "    mask=torch.tril(torch.ones((seq_length,seq_length),device=device)).unsqueeze(0).unsqueeze(1)\n",
        "    for layer in self.layers:\n",
        "      x=layer(x,mask)\n",
        "\n",
        "    return self.fc_out(x)"
      ],
      "metadata": {
        "id": "BM00_LDk19pa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "PiqFMcr5di8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size=encoding.n_vocab"
      ],
      "metadata": {
        "id": "DaGhGQ_teyx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=TransformerBlock(50257,128,100,4,256,4).to(device)"
      ],
      "metadata": {
        "id": "FXDgXcsF8uK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr=1e-3\n",
        "epochs=10"
      ],
      "metadata": {
        "id": "b59YqnTxF0f9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer=torch.optim.Adam(model.parameters(),lr=lr)\n",
        "criterion=nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "AbpM0Yv1FkPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total parameters: {num_params}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJE3HqHG-IrP",
        "outputId": "58d95bd0-7c42-461f-f138-886f974e4c6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total parameters: 13458769\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for xb, yb in dataloader:\n",
        "    print(\"Input batch shape:\", xb.shape)\n",
        "    print(\"Target batch shape:\", yb.shape)\n",
        "    print(model(xb).shape)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1ZS8VhMeCt0",
        "outputId": "cc6b7005-f270-4098-eb5d-86ca399553e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input batch shape: torch.Size([2, 32])\n",
            "Target batch shape: torch.Size([2, 32])\n",
            "torch.Size([2, 32, 50257])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 1\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for xb, yb in tqdm(dataloader):\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(xb)\n",
        "        loss = criterion(logits.view(-1,vocab_size ), yb.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1} Loss: {total_loss/len(dataloader):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HO3uWnO3erxF",
        "outputId": "8eb7951a-fca6-47e4-c3ad-8b31fa53064f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 422/422 [01:54<00:00,  3.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Loss: 0.2299\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, start_text, max_new_tokens=50):\n",
        "    model.eval()\n",
        "    tokens = encoding.encode(start_text)\n",
        "    tokens = torch.tensor(tokens, dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        logits = model(tokens)\n",
        "        next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
        "        if next_token.item() == eot_token:\n",
        "            break\n",
        "        tokens = torch.cat([tokens, next_token], dim=1)\n",
        "\n",
        "    return encoding.decode(tokens[0].tolist())\n",
        "\n",
        "# Example generation\n",
        "print(generate(model, \"Early stopping\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6TLlxo9Ae6al",
        "outputId": "92994556-f73a-40b0-a125-74e6edbcdab4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping prevents overfitting\n"
          ]
        }
      ]
    }
  ]
}